{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "import torch\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from utils.get_opt import get_opt\n",
    "from models.vae.model import VAE\n",
    "from models.denoiser.model import Denoiser\n",
    "from models.denoiser.trainer import DenoiserTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vae(vae_opt):\n",
    "    print(f'Loading VAE Model {vae_opt.name}')\n",
    "\n",
    "    model = VAE(vae_opt)\n",
    "    ckpt = torch.load(pjoin(vae_opt.checkpoints_dir, vae_opt.dataset_name, vae_opt.name, 'model', 'net_best_fid.tar'),\n",
    "                            map_location='cpu')\n",
    "    model.load_state_dict(ckpt[\"vae\"])\n",
    "    model.freeze()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_denoiser(opt, vae_dim):\n",
    "    print(f'Loading Denoiser Model {opt.name}')\n",
    "    denoiser = Denoiser(opt, vae_dim)\n",
    "    ckpt = torch.load(pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name, 'model', 'net_best_fid.tar'),\n",
    "                            map_location='cpu')\n",
    "    missing_keys, unexpected_keys = denoiser.load_state_dict(ckpt[\"denoiser\"], strict=False)\n",
    "\n",
    "    assert len(unexpected_keys) == 0, f'Unexpected keys in denoiser model: {unexpected_keys}'\n",
    "    assert all([k.startswith('clip_model.') for k in missing_keys]), f'Missing keys in denoiser model: {missing_keys}'\n",
    "    \n",
    "    denoiser.to(opt.device)\n",
    "    return denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these two names below as you want\n",
    "dataset_name = \"t2m\"\n",
    "denoiser_name = \"t2m_denoiser_vpred_vaegelu\" # << Change this to use different denoisers\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "opt = get_opt(pjoin(\"checkpoints\", dataset_name, denoiser_name, \"opt.txt\"), device)\n",
    "vae_opt = get_opt(pjoin(\"checkpoints\", dataset_name, opt.vae_name, \"opt.txt\"), device)\n",
    "\n",
    "opt.num_inference_timesteps = 50 # << Change the diffusion timesteps for generation as you want\n",
    "\n",
    "# load models and scheduler\n",
    "vae_model = load_vae(vae_opt).to(opt.device)\n",
    "denoiser = load_denoiser(opt, vae_opt.latent_dim)\n",
    "scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=opt.num_train_timesteps,\n",
    "    beta_start=opt.beta_start,\n",
    "    beta_end=opt.beta_end,\n",
    "    beta_schedule=opt.beta_schedule,\n",
    "    prediction_type=opt.prediction_type,\n",
    "    clip_sample=False,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = DenoiserTrainer(opt, denoiser, vae_model, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import attn2img\n",
    "import numpy as np\n",
    "from utils.fixseed import fixseed\n",
    "\n",
    "fixseed(42)\n",
    "\n",
    "# inputs for generation\n",
    "text = [\n",
    "    \"a man jumps forward and then walks forward, while raising the arms to the sides\",\n",
    "]\n",
    "m_lens = torch.tensor([100]).to(device) # << Change the length of the motion sequence as you want\n",
    "motion = torch.FloatTensor(1, m_lens[0].item(), 263 if dataset_name == \"t2m\" else 251).to(device)\n",
    "\n",
    "assert m_lens % 4 == 0\n",
    "\n",
    "# setup output directory\n",
    "os.makedirs(\"exp_attn\", exist_ok=True)\n",
    "motion, attns = trainer.generate((text, motion, m_lens), need_attn=True)\n",
    "skel_attn, temp_attn, cross_attn = attns\n",
    "\n",
    "# attn map\n",
    "attn_vis = torch.mean(cross_attn, dim=(1, 2, 3))[1]\n",
    "attn_vis = attn_vis.reshape(m_lens[0].item() // 4, 7, 77).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.motion_process import recover_from_ric\n",
    "from utils.plot_script import plot_3d_motion_attn, plot_3d_motion\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def plot_t2m(data, text):\n",
    "    data = data[:m_lens[0].item()]\n",
    "    joint = recover_from_ric(torch.from_numpy(data).float(), opt.joints_num).numpy()\n",
    "    joint = gaussian_filter1d(joint, 1, axis=0)\n",
    "    save_path = pjoin(\"exp_attn\", \"video.mp4\")\n",
    "    plot_3d_motion(save_path, opt.kinematic_chain, joint, title=text, fps=20)\n",
    "    np.save(pjoin(\"exp_attn\", \"motion.npy\"), joint)\n",
    "    \n",
    "    \n",
    "def plot_t2m_attn(data, text, attn_map, idx):\n",
    "    data = data[:m_lens[0].item()]\n",
    "    joint = recover_from_ric(torch.from_numpy(data).float(), opt.joints_num).numpy()\n",
    "    save_path = pjoin(\"exp_attn\", f\"{idx:02d}-{text}.mp4\")\n",
    "    plot_3d_motion_attn(save_path, opt.kinematic_chain, joint, text, attn_map, fps=20)\n",
    "\n",
    "\n",
    "# mean and std for de-normalization\n",
    "wrapper_opt = get_opt(opt.dataset_opt_path, torch.device('cuda'))\n",
    "mean = np.load(pjoin(wrapper_opt.meta_dir, 'mean.npy'))\n",
    "std = np.load(pjoin(wrapper_opt.meta_dir, 'std.npy'))\n",
    "\n",
    "motion_np = motion.detach().cpu().numpy() * std + mean\n",
    "\n",
    "# text info\n",
    "tokens = denoiser.clip_model.tokenize(text)\n",
    "word_emb, mask, max_id = denoiser.clip_model.encode_text(text)\n",
    "\n",
    "# plot\n",
    "print(text)\n",
    "os.system(\"rm exp_attn/*.png\")\n",
    "os.makedirs(\"exp_attn\", exist_ok=True)\n",
    "plot_t2m(motion_np[0], text[0])\n",
    "\n",
    "from IPython.display import display\n",
    "for j in range(max_id[0].item()):\n",
    "    text_decoded0 = denoiser.clip_model.decode_text_from_tokens(tokens.input_ids[0][j])\n",
    "    text_decoded1 = denoiser.clip_model.decode_text_from_tokens(tokens.input_ids[0][j+1])\n",
    "    attn = attn2img(attn_vis[:, :, j], pjoin(\"exp_attn\", f\"attn-{j:02d}-{text_decoded0}.png\"), title=text_decoded0)\n",
    "    # attn = attn2img(attn_vis[:, :, j] + attn_vis[:, :, j+1], pjoin(\"exp_attn\", f\"attn-{j:02d}-{text_decoded0}-{text_decoded1}.png\"), title=text_decoded0 + \" \" + text_decoded1)\n",
    "    display(attn)\n",
    "    # plot_t2m_attn(motion_np[0], text_decoded, torch.repeat_interleave(attn_vis[:, :, j].transpose(0,1), 4, dim=0), j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
